{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81f73c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j.Level\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.storage.StorageLevel\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.Level\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0727de66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ff: (words: Array[String], letters: String)Unit\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ff(words : Array[String], letters : String) : Unit = {\n",
    "        \n",
    "    for (i <- words) {\n",
    "        \n",
    "        i.replace(letters, \" \")\n",
    "        println(i)\n",
    "    }\n",
    "     \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3518ac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5c81b118\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(s\"YEllow\")\n",
    "    .config(\"spark.driver,memory\", \"12g\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b7b4978",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.43.232 executor driver): org.apache.spark.repl.RemoteClassLoaderError: org.apache.spark.sql.catalyst.expressions.Object",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.43.232 executor driver): org.apache.spark.repl.RemoteClassLoaderError: org.apache.spark.sql.catalyst.expressions.Object",
      "\tat org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:120)",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:405)",
      "\tat org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)",
      "\tat java.lang.Class.forName0(Native Method)",
      "\tat java.lang.Class.forName(Class.java:348)",
      "\tat org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)",
      "\tat org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:317)",
      "\tat org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8618)",
      "\tat org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6771)",
      "\tat org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6620)",
      "\tat org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6599)",
      "\tat org.codehaus.janino.UnitCompiler.access$14300(UnitCompiler.java:226)",
      "\tat org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6502)",
      "\tat org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6497)",
      "\tat org.codehaus.janino.Java$ReferenceType.accept(Java.java:4134)",
      "\tat org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6497)",
      "\tat org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6490)",
      "\tat org.codehaus.janino.Java$ReferenceType.accept(Java.java:4133)",
      "\tat org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6490)",
      "\tat org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6895)",
      "\tat org.codehaus.janino.UnitCompiler.access$14100(UnitCompiler.java:226)",
      "\tat org.codehaus.janino.UnitCompiler$22$1.visitArrayType(UnitCompiler.java:6500)",
      "\tat org.codehaus.janino.UnitCompiler$22$1.visitArrayType(UnitCompiler.java:6497)",
      "\tat org.codehaus.janino.Java$ArrayType.accept(Java.java:4215)",
      "\tat org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6497)",
      "\tat org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6490)",
      "\tat org.codehaus.janino.Java$ArrayType.accept(Java.java:4214)",
      "\tat org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6490)",
      "\tat org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:226)",
      "\tat org.codehaus.janino.UnitCompiler$36.getParameterTypes2(UnitCompiler.java:10451)",
      "\tat org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:959)",
      "\tat org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1224)",
      "\tat org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:982)",
      "\tat org.codehaus.janino.IClass.getIMethods(IClass.java:248)",
      "\tat org.codehaus.janino.IClass.getIMethods(IClass.java:237)",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:470)",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)",
      "\tat org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)",
      "\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)",
      "\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)",
      "\tat org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)",
      "\tat org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)",
      "\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)",
      "\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)",
      "\tat org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)",
      "\tat org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)",
      "\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)",
      "\tat org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)",
      "\tat org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)",
      "\tat org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)",
      "\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)",
      "\tat org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1489)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)",
      "\tat org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1436)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:378)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:331)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:34)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1362)",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1359)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.appendPartitionColumns$lzycompute(FileFormat.scala:138)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.appendPartitionColumns(FileFormat.scala:137)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:142)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:133)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "Caused by: java.io.IOException: Failed to connect to /192.168.43.232:52945",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:399)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$openChannel$4(NettyRpcEnv.scala:367)",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:366)",
      "\tat org.apache.spark.repl.ExecutorClassLoader.getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:135)",
      "\tat org.apache.spark.repl.ExecutorClassLoader.$anonfun$fetchFn$1(ExecutorClassLoader.scala:66)",
      "\tat org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:176)",
      "\tat org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:113)",
      "\t... 96 more",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.43.232:52945",
      "Caused by: java.net.ConnectException: Operation timed out",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2935)",
      "  at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:112)",
      "  at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:65)",
      "  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)",
      "  at scala.Option.orElse(Option.scala:447)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)",
      "  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:481)",
      "  ... 41 elided",
      "Caused by: org.apache.spark.repl.RemoteClassLoaderError: org.apache.spark.sql.catalyst.expressions.Object",
      "  at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:120)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:418)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:405)",
      "  at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:351)",
      "  at java.lang.Class.forName0(Native Method)",
      "  at java.lang.Class.forName(Class.java:348)",
      "  at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)",
      "  at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:317)",
      "  at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8618)",
      "  at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6771)",
      "  at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6620)",
      "  at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6599)",
      "  at org.codehaus.janino.UnitCompiler.access$14300(UnitCompiler.java:226)",
      "  at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6502)",
      "  at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6497)",
      "  at org.codehaus.janino.Java$ReferenceType.accept(Java.java:4134)",
      "  at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6497)",
      "  at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6490)",
      "  at org.codehaus.janino.Java$ReferenceType.accept(Java.java:4133)",
      "  at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6490)",
      "  at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6895)",
      "  at org.codehaus.janino.UnitCompiler.access$14100(UnitCompiler.java:226)",
      "  at org.codehaus.janino.UnitCompiler$22$1.visitArrayType(UnitCompiler.java:6500)",
      "  at org.codehaus.janino.UnitCompiler$22$1.visitArrayType(UnitCompiler.java:6497)",
      "  at org.codehaus.janino.Java$ArrayType.accept(Java.java:4215)",
      "  at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6497)",
      "  at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6490)",
      "  at org.codehaus.janino.Java$ArrayType.accept(Java.java:4214)",
      "  at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6490)",
      "  at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:226)",
      "  at org.codehaus.janino.UnitCompiler$36.getParameterTypes2(UnitCompiler.java:10451)",
      "  at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:959)",
      "  at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1224)",
      "  at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:982)",
      "  at org.codehaus.janino.IClass.getIMethods(IClass.java:248)",
      "  at org.codehaus.janino.IClass.getIMethods(IClass.java:237)",
      "  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:470)",
      "  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)",
      "  at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)",
      "  at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)",
      "  at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)",
      "  at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)",
      "  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)",
      "  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)",
      "  at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)",
      "  at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)",
      "  at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)",
      "  at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)",
      "  at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)",
      "  at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)",
      "  at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)",
      "  at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)",
      "  at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)",
      "  at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)",
      "  at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1489)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)",
      "  at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)",
      "  at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)",
      "  at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)",
      "  at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)",
      "  at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)",
      "  at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)",
      "  at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1436)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:378)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:331)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:34)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1362)",
      "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1359)",
      "  at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.appendPartitionColumns$lzycompute(FileFormat.scala:138)",
      "  at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.appendPartitionColumns(FileFormat.scala:137)",
      "  at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:142)",
      "  at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:133)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      "Caused by: java.io.IOException: Failed to connect to /192.168.43.232:52945",
      "  at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)",
      "  at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)",
      "  at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)",
      "  at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:399)",
      "  at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$openChannel$4(NettyRpcEnv.scala:367)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)",
      "  at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:366)",
      "  at org.apache.spark.repl.ExecutorClassLoader.getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:135)",
      "  at org.apache.spark.repl.ExecutorClassLoader.$anonfun$fetchFn$1(ExecutorClassLoader.scala:66)",
      "  at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:176)",
      "  at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:113)",
      "  ... 96 more",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.43.232:52945",
      "Caused by: java.net.ConnectException: Operation timed out",
      "  at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
      "  at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)",
      "  at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)",
      "  at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)",
      "  at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)",
      "  at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)",
      "  at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)",
      "  at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)",
      "  at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)",
      "  at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)",
      "  at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)",
      "  at java.lang.Thread.run(Thread.java:748)",
      ""
     ]
    }
   ],
   "source": [
    "val path = \"/Users/diakite/Downloads/Telegram Desktop/GL_TRX_INTERFACE_20221031.csv\"\n",
    "\n",
    "var df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"|\").csv(path).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89e4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f019dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878500d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a36a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
