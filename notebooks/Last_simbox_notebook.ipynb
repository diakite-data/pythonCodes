{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans,BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from itertools import chain\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "          .appName(\"sunshine_v2\")\\\n",
    "          .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction des detections communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cette fonction retrourne un df conteant les détections communes (FRA et sunshine)\n",
    "## elle prend en entrée une date sous le format (yyyyMMdd)\n",
    "## la liste des fraudeurs va du 1er au 22 Décembre 2021\n",
    "def commun1(date) :\n",
    "        dec_1_22 = spark.read.option(\"header\", True) \\\n",
    "                        .option(\"delimiter\", \",\") \\\n",
    "                        .csv(\"/Users/youssouf/Downloads/SIMBOX_SUNSHINE_DEC_0122.csv\") \\\n",
    "                        .withColumn(\"date_detection\", \n",
    "                                         date_format(to_date(col(\"Date_DETECTION\"), \"yyyy-MM-dd\"), \n",
    "                                                     \"yyyyMMdd\"))  \\\n",
    "                        .withColumnRenamed(\"MSISDN_SB\", \"msisdn\")  \\\n",
    "                        .filter(col(\"date_detection\") == date)               \n",
    "        #dec_1_22.show(2)      \n",
    "        path_prediction = f\"/Users/youssouf/Downloads/prediction_rf_{date}.csv\"\n",
    "        prediction_01 = spark.read.option(\"header\", True) \\\n",
    "                        .option(\"delimiter\", \";\") \\\n",
    "                        .csv(path_prediction)      \n",
    "        #prediction_01.show(2) \n",
    "        cols = [\"date_appel\", \"distributeur\", \"nbre_fois_detectes\", \"moyenne_probability\" , \"probability\", \"msisdn\"]\n",
    "        commun = prediction_01.join(dec_1_22, [\"msisdn\"], \"inner\").select(*(c for c in cols))\n",
    "        return commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "|date_appel|distributeur|nbre_fois_detectes|moyenne_probability|      probability|    msisdn|\n",
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "|  20211201|         SII|                 1|   0.65886954361059| 0.65886954361059|0703152224|\n",
      "|  20211201|         AJS|                 1|  0.718101178241166|0.718101178241166|0703624390|\n",
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commun1(\"20211201\").show(2) ## j'obtiens ici les détections communes de la journée 20211201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de calcul des Intervalles de Confiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cette fonction retourne un intervalle de confiance basée sur les probabilités communes \n",
    "## l'intervalle de conficance est calculée pour sigma inconnue et on cherche à estimer la moyenne\n",
    "## elle prend en paramètres le jour et alpha\n",
    "def confidence_interval(day, alpha) :    \n",
    "    stats = commun1(day).toPandas()[\"probability\"].astype(float).to_list()\n",
    "    alpha = alpha\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = np.percentile(stats, p)\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = np.percentile(stats, p)\n",
    "    print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0 confidence interval 53.2% and 77.8%\n"
     ]
    }
   ],
   "source": [
    "confidence_interval(\"20211201\", 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0 confidence interval 51.0% and 83.6%\n"
     ]
    }
   ],
   "source": [
    "confidence_interval(\"20211202\", 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cette fonction ci est pareille que *confidence_interval* juste qu'ellle prend en paramètres un df(chargé en pyspark) \n",
    "## et alpha\n",
    "def confidence_interval1(df, alpha) :    \n",
    "    stats = df.toPandas()[\"probability\"].astype(float).to_list()\n",
    "    alpha = alpha\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = np.percentile(stats, p)\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = np.percentile(stats, p)\n",
    "    print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))  \n",
    "    return [lower,upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction des union des df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 fonctions *ar* et *union_commun_pred*\n",
    "## *ar* juste une fonction intermédiare\n",
    "## *union_commun_pred* retourne les détections communes(FRA - SUNSHINE) et les predictions, elle prend en paramètre le nombre de jour \n",
    "## à unir...\n",
    "def ar(x) :\n",
    "    if len(x) <= 1 :\n",
    "        return \"0\" + x\n",
    "    else :\n",
    "        return x\n",
    "\n",
    "def union_commun_pred(nb) :\n",
    "    \n",
    "    list_a = [str(x)  for x in list(range(1,13))]\n",
    "    lst_day = [ar(x) for x in list_a]\n",
    "    \n",
    "    \n",
    "    L_commun = []\n",
    "    L_pred = []\n",
    "    \n",
    "    for day in lst_day :\n",
    "        dy = f\"202112{day}\"\n",
    "        path_prediction = f\"/Users/youssouf/Downloads/prediction_rf_202112{day}.csv\"\n",
    "        L_commun.append(commun1(dy))\n",
    "        prediction = spark.read.option(\"header\", True) \\\n",
    "                        .option(\"delimiter\", \";\") \\\n",
    "                        .csv(path_prediction) \\\n",
    "                        .select(\"msisdn\",\"probability\", \"date_appel\")\n",
    "        L_pred.append(prediction)\n",
    "        \n",
    "    #union_df =  L_df.reduce((a, b) => a.union(b))  \n",
    "    union_commun = reduce(DataFrame.union , L_commun)\n",
    "    union_pred = reduce(DataFrame.union , L_pred)   \n",
    "    return [union_commun, union_pred]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "|date_appel|distributeur|nbre_fois_detectes|moyenne_probability|      probability|    msisdn|\n",
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "|  20211201|         SII|                 1|   0.65886954361059| 0.65886954361059|0703152224|\n",
      "|  20211201|         AJS|                 1|  0.718101178241166|0.718101178241166|0703624390|\n",
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_commun_pred(2)[0].show(2)  # communs 2 jours (20211201 - 20211202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+\n",
      "|    msisdn|       probability|date_appel|\n",
      "+----------+------------------+----------+\n",
      "|0700004911|0.5160806693366838|  20211201|\n",
      "|0700006491|0.6706355589116454|  20211201|\n",
      "+----------+------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_commun_pred(2)[1].show(2)  # predictions 2 jours (20211201 - 20211202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "|date_appel|distributeur|nbre_fois_detectes|moyenne_probability|      probability|    msisdn|\n",
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "|  20211201|         SII|                 1|   0.65886954361059| 0.65886954361059|0703152224|\n",
      "|  20211201|         AJS|                 1|  0.718101178241166|0.718101178241166|0703624390|\n",
      "+----------+------------+------------------+-------------------+-----------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_commun_pred(3)[0].show(2)  # communs 3 jours (20211201 - 20211202 - 20211203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+\n",
      "|    msisdn|       probability|date_appel|\n",
      "+----------+------------------+----------+\n",
      "|0700004911|0.5160806693366838|  20211201|\n",
      "|0700006491|0.6706355589116454|  20211201|\n",
      "+----------+------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_commun_pred(3)[1].show(2)  # predictions 3 jours (20211201 - 20211202 - 20211203)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de filtre en utilisant les bornes des intervalles de confiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cette fonction-ci calcule l'IC et ensuite applique le filtre sur le df de prediction et \n",
    "## le df des detections commnes\n",
    "## elle retourne le nombre d'individus obtenus après et avant application du filtre \n",
    "## elle prend en entrée le jour et alpha\n",
    "\n",
    "def cf_filter2(date, alpha) :\n",
    "\n",
    "        dec_1_22 = spark.read.option(\"header\", True) \\\n",
    "                        .option(\"delimiter\", \",\") \\\n",
    "                        .csv(\"/Users/youssouf/Downloads/SIMBOX_SUNSHINE_DEC_0122.csv\") \\\n",
    "                        .withColumn(\"date_detection\", \n",
    "                                         date_format(to_date(col(\"Date_DETECTION\"), \"yyyy-MM-dd\"), \n",
    "                                                     \"yyyyMMdd\"))  \\\n",
    "                        .withColumnRenamed(\"MSISDN_SB\", \"msisdn\")  \\\n",
    "                        .filter(col(\"date_detection\") == date)\n",
    "                        \n",
    "        #dec_1_22.show(2)\n",
    "               \n",
    "        path_prediction = f\"/Users/youssouf/Downloads/prediction_rf_{date}.csv\"\n",
    "        prediction = spark.read.option(\"header\", True) \\\n",
    "                        .option(\"delimiter\", \";\") \\\n",
    "                        .csv(path_prediction) \n",
    "                \n",
    "        #prediction_01.show(2)\n",
    "        \n",
    "        cols = [\"date_appel\", \"distributeur\", \"nbre_fois_detectes\", \"moyenne_probability\" , \"probability\", \"msisdn\"]\n",
    "        commun = prediction.join(dec_1_22, [\"msisdn\"], \"inner\").select(*(c for c in cols))\n",
    "        \n",
    "        c = commun.distinct().count()                \n",
    "        born = confidence_interval1(commun, alpha)\n",
    "        lower,upper = born[0],born[1]\n",
    "        #c1 = commun.filter( (col(\"probability\") >= lower) & (col(\"probability\") <= upper) )\n",
    "        \n",
    "        prediction_f = prediction.filter( (col(\"probability\") >= lower) & (col(\"probability\") <= upper) )\n",
    "        commun_f = prediction_f.join(dec_1_22, [\"msisdn\"], \"inner\").select(*(c for c in cols))\n",
    "        c1 = commun_f.distinct().count()\n",
    "             \n",
    "        return {\"BEFORE\" : [c,prediction.distinct().count()] , \"AFTER\" : [c1,prediction_f.distinct().count()] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0 confidence interval 52.7% and 77.8%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BEFORE': [146, 980611], 'AFTER': [145, 534987]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_filter2(\"20211201\", 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0 confidence interval 52.0% and 83.5%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BEFORE': [130, 660417], 'AFTER': [126, 430834]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_filter2(\"20211202\", 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cette fonction est pareille à *cf_filter* mais c'est juste qu'elle s'applique seulement que sur l'union des df \n",
    "## (commun - prediction)\n",
    "## elle prend en paramètres le nombre de jour à unir(nb) et alpha\n",
    "def cf_filter3(nb, alpha) :  \n",
    "    u = union_commun_pred(nb)\n",
    "    commun = u[0]\n",
    "    pred = u[1]\n",
    "    print(\" ***** COMMUN ***** \")\n",
    "    born = confidence_interval1(commun, alpha)\n",
    "    lower,upper = born[0],born[1]\n",
    "    c = commun.distinct().count()\n",
    "    c1 = commun.filter( (col(\"probability\") >= lower) & (col(\"probability\") <= upper) ).distinct().count()      \n",
    "    print(\" ***** PRED ***** \")\n",
    "    born = confidence_interval1(pred, alpha)\n",
    "    lower,upper = born[0],born[1]\n",
    "    p = pred.distinct().count()\n",
    "    p1 = pred.filter( (col(\"probability\") >= lower) & (col(\"probability\") <= upper) ).distinct().count()\n",
    "    return {\"PRED\" :  {\"BEFORE\" : c, \"AFTER\" : c1}, \"COMMUN\" :  {\"BEFORE\" : p, \"AFTER\" : p1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
